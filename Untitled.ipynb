{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28648a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fa016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "#import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.io import read_file,decode_jpeg\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.random import uniform\n",
    "from tensorflow.errors import InvalidArgumentError\n",
    "from tensorflow.math import abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3faa63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = read_file(file_path)\n",
    "    try:\n",
    "        img = decode_jpeg(byte_img)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        img = uniform([250, 250, 3], minval=0, maxval=256)\n",
    "    img = resize(img,(100,100))\n",
    "    img = img/255.0\n",
    "    return img\n",
    "\n",
    "def prediction(input_img_path,val_img_path):\n",
    "    input_img = preprocess(input_img_path)\n",
    "    val_img = preprocess(val_img_path)\n",
    "    result = model.predict(list(np.expand_dims([input_img,val_img],axis=1)))\n",
    "    print(result)\n",
    "    if result>0.5:\n",
    "        print('')\n",
    "        return 'Not Misplaced'\n",
    "    else:\n",
    "        print('Clamp is misplaced')\n",
    "        return 'Misplaced'\n",
    "\n",
    "\n",
    "## Reload the model\n",
    "## Not using the custom_objects will trigger errors\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return abs(input_embedding-validation_embedding)\n",
    "\n",
    "binary_cross_loss = BinaryCrossentropy()\n",
    "\n",
    "model = load_model('multiple_classes_defect_detection.h5',\n",
    "                    custom_objects={'L1Dist':L1Dist,'binary_cross_loss':BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c9c37dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Screenshot 2023-12-18 152829.jpg',\n",
       " 'Screenshot 2023-12-18 152902.jpg',\n",
       " 'Screenshot 2023-12-18 152921.jpg']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../two Crop/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c8d2075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img_path = '../two Crop/Screenshot 2023-12-18 152921.jpg'\n",
    "val_img_path = 'multi_classes_images/two.jpeg'\n",
    "input_img = preprocess(input_img_path)\n",
    "val_img = preprocess(val_img_path)\n",
    "model.predict(list(np.expand_dims([input_img,val_img],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "['fire_extinguisher',\n",
    " 'grid',\n",
    " 'layout',\n",
    " 'power_eco',\n",
    " 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "036e7247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fire_extinguisher.jpeg',\n",
       " 'grid.jpeg',\n",
       " 'layout.jpeg',\n",
       " 'power_eco.jpeg',\n",
       " 'top.jpeg']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('multi_classes_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "af11aa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[2.1646831e-15]]\n",
      "Clamp is misplaced\n",
      "Misplaced\n"
     ]
    }
   ],
   "source": [
    "temp_filename = os.listdir('uploaded_images')[0]\n",
    "\n",
    "# You can save the uploaded file to a directory.\n",
    "# with open(f\"uploaded_images/{temp_filename}\", \"wb\") as f:\n",
    "#     f.write(file.file.read())\n",
    "    \n",
    "input_img_type = \"top\"\n",
    "img_extension = \".jpeg\"\n",
    "input_img_filename = input_img_type + img_extension\n",
    "\n",
    "input_img_path = f\"uploaded_images/{temp_filename}\"\n",
    "val_img_path = f\"multi_classes_images/{input_img_filename}\"\n",
    "result = prediction(input_img_path,val_img_path)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf2464e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = preprocess(input_img_path)\n",
    "val_img = preprocess(val_img_path)\n",
    "#result = model.predict(list(np.expand_dims([input_img,val_img],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a027b342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uploaded_images/top 3_3.jpeg'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "331bf7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = input_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "42120d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 100, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "byte_img = read_file(file_path)\n",
    "try:\n",
    "    img = decode_jpeg(byte_img)\n",
    "except Exception as e:\n",
    "    img = uniform([250, 250, 3], minval=0, maxval=256)\n",
    "    print('Created an (250,250,3) array as input image was empty')\n",
    "img = resize(img,(100,100))\n",
    "img = img/255.0\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2038fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_image (InputLayer)  [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_image[0][0]',            \n",
      "                                                                  'validation_image[0][0]']       \n",
      "                                                                                                  \n",
      " l1_dist_2 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['l1_dist_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5cfa71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"siamese_network\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " validation_image (InputLayer)  [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " embedding (Functional)         (None, 4096)         38960448    ['input_image[0][0]',            \n",
      "                                                                  'validation_image[0][0]']       \n",
      "                                                                                                  \n",
      " l1_dist_3 (L1Dist)             (None, 4096)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding[1][0]']              \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            4097        ['l1_dist_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,964,545\n",
      "Trainable params: 38,964,545\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = load_model('clamp_defect_detection.h5',\n",
    "                    custom_objects={'L1Dist':L1Dist,'binary_cross_loss':BinaryCrossentropy})\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save the uploaded file to a directory.\n",
    "with open(f\"uploaded_images/{file.filename}\", \"wb\") as f:\n",
    "    f.write(file.file.read())\n",
    "    \n",
    "input_img_type = \"top\"\n",
    "img_extension = '.jpeg'\n",
    "input_img_filename = input_img_type + img_extension\n",
    "\n",
    "input_img_path = f\"uploaded_images/{file.filename}\"\n",
    "val_img_path = f\"multi_classes_images/{input_img_filename}\"\n",
    "result = prediction(input_img_path,val_img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
